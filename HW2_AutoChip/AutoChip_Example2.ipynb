{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIDfyasywGS3"
      },
      "source": [
        "# Example 2 — Sequence Detector (FSM)\n",
        "\n",
        "## Origin\n",
        "This example is taken directly from **ChipChat Example 2** (`sequence_detector`).\n",
        "In ChipChat, the task was posed as a natural-language question to the LLM:\n",
        "\n",
        "> *\"I am trying to create a Verilog model for a sequence detector. It must meet the following specifications:*\n",
        "> - *Inputs: Clock, Active-low reset, Data (3 bits)*\n",
        "> - *Outputs: Sequence found*\n",
        ">\n",
        "> *While enabled, it should detect the following sequence of binary input values:*\n",
        "> `001 → 101 → 110 → 000 → 110 → 110 → 011 → 101`\n",
        ">\n",
        "> *How would I write a design that meets these specifications?\"*\n",
        "\n",
        "## Testbench\n",
        "The testbench `sequence_detector_tb.v` was taken directly from the course assignment\n",
        "repository (`LLM4ChipDesign/VerilogGenBenchmark/TestBench/`) as provided in the\n",
        "ChipChat Colab and required by the AutoChip Tutorial Assignment. It is the official course-supplied testbench for this module.\n",
        "\n",
        "## Why AutoChip instead of ChipChat\n",
        "AutoChip improves on the ChipChat workflow by:\n",
        "1. Running **multiple candidate RTL responses per iteration** (3 here) and ranking them\n",
        "2. Automatically feeding **compiler and simulation errors back** to the LLM as structured feedback\n",
        "3. Iterating until a candidate achieves **rank 1.0** (testbench prints \"All test cases passed.\")\n",
        "\n",
        "## Module specification\n",
        "- **Module name:** `sequence_detector`\n",
        "- **Inputs:** `clk`, `reset_n` (active-low synchronous), `data [2:0]`\n",
        "- **Output:** `sequence_found` (reg) — asserted for exactly 1 cycle when full sequence received\n",
        "- **Logic type:** Sequential FSM, 8 states S0–S7\n",
        "- **Sequence:** `001 → 101 → 110 → 000 → 110 → 110 → 011 → 101`\n",
        "- **Critical constraint:** `sequence_found` must be asserted in the **same clock cycle** the final `101` is received in S7\n",
        "- **Constraint:** Plain Verilog-2001 only — no `logic`, `typedef`, `enum`, `always_ff`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsbbXgjNAdy9"
      },
      "source": [
        "# Getting set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G-ll5GZeKMwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd648a2-e563-40fb-a602-8139ec493853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,688 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,717 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,611 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,049 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,300 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,477 kB]\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Get:19 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,744 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,903 kB]\n",
            "Get:23 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Fetched 37.3 MB in 4s (9,123 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  gtkwave\n",
            "The following NEW packages will be installed:\n",
            "  iverilog\n",
            "0 upgraded, 1 newly installed, 0 to remove and 66 not upgraded.\n",
            "Need to get 2,130 kB of archives.\n",
            "After this operation, 6,749 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
            "Fetched 2,130 kB in 1s (2,805 kB/s)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 117540 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Setting up the notebook\n",
        "\n",
        "### Installing dependencies\n",
        "!pip install openai tiktoken\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Recommended: Runtime > Secrets > add 'openai_api_key', then uncomment below\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"  # replace before running"
      ],
      "metadata": {
        "id": "lrRtp6ApM0It"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lk5cP5x12z9u"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "from abc import ABC, abstractmethod\n",
        "import re\n",
        "import getopt\n",
        "import json\n",
        "\n",
        "################################################################################\n",
        "### LOGGING\n",
        "################################################################################\n",
        "# Allows us to log the output of the model to a file if logging is enabled\n",
        "class LogStdoutToFile:\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._original_stdout = sys.stdout\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self._filename:\n",
        "            sys.stdout = open(self._filename, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._filename:\n",
        "            sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "################################################################################\n",
        "### CONFIG & ARGS\n",
        "################################################################################\n",
        "def load_config(config_file=\"config.json\"):\n",
        "    \"\"\"Load and validate the configuration from the specified JSON file.\"\"\"\n",
        "    with open(config_file, 'r') as file:\n",
        "        config = json.load(file)\n",
        "\n",
        "    if 'general' not in config:\n",
        "        raise ValueError(\"Missing general section in config file\")\n",
        "\n",
        "    config_values = config['general']\n",
        "\n",
        "    # Only parse ensemble settings if specified\n",
        "    parse_ensemble = config_values.get('ensemble', False)\n",
        "    ensemble_config = {}\n",
        "    if parse_ensemble:\n",
        "        ensemble_config = config.get('ensemble', {})\n",
        "\n",
        "    #return config_values\n",
        "    return config_values, ensemble_config\n",
        "\n",
        "\n",
        "def validate_ensemble_config(ensemble_config, max_iterations):\n",
        "    seen_start_iterations = set()\n",
        "    adjusted_config = {}\n",
        "    has_start_at_zero = False\n",
        "\n",
        "    for model_name, model_info in ensemble_config.items():\n",
        "        start_iteration = model_info['start_iteration']\n",
        "\n",
        "        # Adjust negative start_iteration values\n",
        "        if start_iteration < 0:\n",
        "            start_iteration += max_iterations+1\n",
        "\n",
        "        # Check if start_iteration is within the valid range\n",
        "        if not (0 <= start_iteration <= max_iterations):\n",
        "            raise ValueError(f\"Invalid start_iteration {model_info['start_iteration']} for {model_name}. \"\n",
        "                             f\"Must be within the range of 0 to {max_iterations} or valid negative index.\")\n",
        "\n",
        "        # Check for conflicting start_iterations\n",
        "        if start_iteration in seen_start_iterations:\n",
        "            raise ValueError(f\"Conflicting start_iteration {start_iteration} for {model_name}. \"\n",
        "                             f\"Another model already uses this start iteration.\")\n",
        "        seen_start_iterations.add(start_iteration)\n",
        "\n",
        "        # Check if there is a model starting at iteration 0\n",
        "        if start_iteration == 0:\n",
        "            has_start_at_zero = True\n",
        "\n",
        "        # Update the adjusted configuration\n",
        "        adjusted_config[model_name] = {\n",
        "            \"start_iteration\": start_iteration,\n",
        "            \"model_family\": model_info['model_family'],\n",
        "            \"model_id\": model_info['model_id']\n",
        "        }\n",
        "\n",
        "        if not has_start_at_zero:\n",
        "            raise ValueError(\"No model starting at iteration 0 in the ensemble. One model must start at iteration 0.\")\n",
        "\n",
        "    return adjusted_config\n",
        "\n",
        "\n",
        "def parse_args_and_config():\n",
        "    \"\"\"Parse command-line arguments and merge them with configuration file values.\"\"\"\n",
        "    usage = \"\"\"Usage: auto_create_verilog.py [--help] --prompt=<prompt> --name=<module name> --testbench=<testbench file> --iter=<iterations> --model=<llm family> --model-id=<specific model> --num-candidates=<candidates per request> --outdir=<directory for outputs> --log=<log file>\n",
        "\n",
        "\t-h|--help: Prints this usage message\n",
        "\n",
        "\t-p|--prompt: The initial design prompt for the Verilog module\n",
        "\n",
        "\t-n|--name: The module name, must match the testbench expected module name\n",
        "\n",
        "\t-t|--testbench: The testbench file to be run\n",
        "\n",
        "\t-i|--iter: [Optional] Number of iterations before the tool quits (defaults to 10)\n",
        "\n",
        "\t-m|--model: The LLM family to use. Must be one of the following\n",
        "\t\t- ChatGPT\n",
        "\t\t- Claude\n",
        "\t\t- Mistral\n",
        "\t\t- Gemini\n",
        "\t\t- CodeLlama\n",
        "\t\t- Human (requests user input)\n",
        "\n",
        "\t--model-id: The specific model to use for the model family\n",
        "\n",
        "\t--num-candidates: The number of candidates to rank per tree level\n",
        "\n",
        "\t-o|--outdir: Directory to place all run-specific files in\n",
        "\n",
        "\t-l|--log: [Optional] Log the output of the model to the given file\n",
        "\"\"\"\n",
        "\n",
        "    config_file = \"config.json\"\n",
        "\n",
        "    # Load config values from the file\n",
        "    config_values, ensemble_config = load_config(config_file)\n",
        "\n",
        "    required_values = ['prompt', 'name', 'testbench', 'outdir', 'log']\n",
        "    if not ensemble_config:\n",
        "        required_values +=['model_family', 'model_id']\n",
        "\n",
        "    for value in required_values:\n",
        "        if value not in config_values:\n",
        "            raise ValueError(f\"Missing {value} in general section\\n{usage}\")\n",
        "\n",
        "\n",
        "    # general values for optional config values\n",
        "    if 'num_candidates' not in config_values:\n",
        "        config_values['num_candidates'] = 1\n",
        "    if 'iterations' not in config_values:\n",
        "        config_values['iterations'] = 10\n",
        "\n",
        "\n",
        "    if ensemble_config:\n",
        "        ensemble_config = validate_ensemble_config(ensemble_config, config_values['iterations'])\n",
        "\n",
        "    # Ensure outdir exists\n",
        "    if config_values['outdir']:\n",
        "        os.makedirs(config_values['outdir'], exist_ok=True)\n",
        "\n",
        "    logfile = os.path.join(config_values['outdir'], config_values['log']) if config_values['log'] else None\n",
        "\n",
        "    #return config_values, logfile\n",
        "    return config_values, ensemble_config, logfile\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### CONVERSATION CLASS\n",
        "# allows us to abstract away the details of the conversation for use with\n",
        "# different LLM APIs\n",
        "################################################################################\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, log_file=None):\n",
        "        self.messages = []\n",
        "        self.log_file = log_file\n",
        "\n",
        "        if self.log_file and os.path.exists(self.log_file):\n",
        "            open(self.log_file, 'w').close()\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Add a new message to the conversation.\"\"\"\n",
        "        self.messages.append({'role': role, 'content': content})\n",
        "\n",
        "        if self.log_file:\n",
        "            with open(self.log_file, 'a') as file:\n",
        "                file.write(f\"{role}: {content}\\n\")\n",
        "\n",
        "    def get_messages(self):\n",
        "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
        "        return self.messages\n",
        "\n",
        "    def get_last_n_messages(self, n):\n",
        "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
        "        return self.messages[-n:]\n",
        "\n",
        "    def remove_message(self, index):\n",
        "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
        "        if index < len(self.messages):\n",
        "            del self.messages[index]\n",
        "\n",
        "    def get_message(self):\n",
        "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
        "        return self.messages[index] if index < len(self.messages) else None\n",
        "\n",
        "    def clear_messages(self):\n",
        "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
        "        self.messages = []\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return the conversation in a string format.\"\"\"\n",
        "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
        "\n",
        "################################################################################\n",
        "### LLM CLASSES\n",
        "# Defines an interface for using different LLMs so we can easily swap them out\n",
        "################################################################################\n",
        "class AbstractLLM(ABC):\n",
        "    \"\"\"Abstract Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, conversation: Conversation, num_candidates=1):\n",
        "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class ChatGPT(AbstractLLM):\n",
        "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id=\"gpt-4o\"):\n",
        "        super().__init__()\n",
        "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
        "        self.client = openai.OpenAI()\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def generate(self, conversation: Conversation, num_candidates=1):\n",
        "        messages = [{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "\n",
        "\n",
        "        #print(f\"model_id: {self.model_id}\")\n",
        "        #print(f\"messages: {messages}\")\n",
        "        #print(f\"num_candidates: {num_candidates}\")\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "        model=self.model_id,\n",
        "        n=num_candidates,\n",
        "        messages=messages,\n",
        "        temperature=0.1\n",
        ")\n",
        "\n",
        "\n",
        "        return [c.message.content for c in response.choices]\n",
        "\n",
        "class LLMResponse():\n",
        "    \"\"\"Class to store the response from the LLM\"\"\"\n",
        "    def __init__(self, iteration, response_num, full_text):\n",
        "        self.iteration = iteration\n",
        "        self.response_num = response_num\n",
        "\n",
        "        self.full_text = full_text\n",
        "        self.tokens = 0\n",
        "\n",
        "        self.parsed_text = \"\"\n",
        "        self.parsed_length = 0\n",
        "\n",
        "        self.feedback = \"\"\n",
        "        self.compiled = False\n",
        "        self.rank = -3\n",
        "        self.message = \"\"\n",
        "\n",
        "    def set_parsed_text(self, parsed_text):\n",
        "        self.parsed_text = parsed_text\n",
        "        self.parsed_length = len(parsed_text)\n",
        "\n",
        "    def parse_verilog(self):\n",
        "        module_list = find_verilog_modules(self.full_text)\n",
        "        if not module_list:\n",
        "            print(\"No modules found in response\")\n",
        "            self.parsed_text = \"\"\n",
        "        else:\n",
        "            for module in module_list:\n",
        "                self.parsed_text += module + \"\\n\\n\"\n",
        "        self.parsed_length = len(self.parsed_text)\n",
        "\n",
        "    def calculate_rank(self, outdir, module, testbench):\n",
        "        filename = os.path.join(outdir, module + \".sv\")\n",
        "        vvp_file = os.path.join(outdir, module + \".vvp\")\n",
        "\n",
        "        compiler_cmd = f\"iverilog -Wall -Winfloop -Wno-timescale -g2012 -s tb_sequence_detector -o {vvp_file} {filename} {testbench}\"\n",
        "        simulator_cmd = f\"vvp -n {vvp_file}\"\n",
        "\n",
        "        try:\n",
        "            comp_return, comp_err, comp_out = compile_iverilog(outdir, module, compiler_cmd, self)\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "            self.rank = -2\n",
        "            return\n",
        "\n",
        "        # -----------------------------\n",
        "        # Compilation handling\n",
        "        # -----------------------------\n",
        "        if comp_return != 0:\n",
        "            self.feedback = comp_err\n",
        "            self.compiled = False\n",
        "            print(\"Compilation error\")\n",
        "            print(comp_err)\n",
        "            self.message = (\n",
        "                \"The design failed to compile. Please fix the module. \"\n",
        "                \"The output of iverilog is as follows:\\n\" + comp_err\n",
        "            )\n",
        "            self.rank = -1\n",
        "            return\n",
        "\n",
        "        elif comp_err != \"\":\n",
        "            self.feedback = comp_err\n",
        "            self.compiled = True\n",
        "            print(\"Compilation warning\")\n",
        "            print(comp_err)\n",
        "            self.message = (\n",
        "                \"The design compiled with warnings. Please fix the module. \"\n",
        "                \"The output of iverilog is as follows:\\n\" + comp_err\n",
        "            )\n",
        "            self.rank = -0.5\n",
        "            return\n",
        "\n",
        "        # -----------------------------\n",
        "        # Simulation handling\n",
        "        # -----------------------------\n",
        "        sim_return, sim_err, sim_out = simulate_iverilog(simulator_cmd)\n",
        "\n",
        "        print(\"Simulation output:\")\n",
        "        print(sim_out)\n",
        "\n",
        "        # Success case (ChipChat testbench style)\n",
        "        if \"All test cases passed.\" in sim_out:\n",
        "            self.compiled = True\n",
        "            self.feedback = \"\"\n",
        "            self.message = \"The testbench completed successfully.\"\n",
        "            self.rank = 1.0\n",
        "            print(\"Testbench ran successfully\")\n",
        "            return\n",
        "\n",
        "        # Failure case\n",
        "        else:\n",
        "            self.compiled = True\n",
        "            self.feedback = sim_out\n",
        "            self.message = (\n",
        "                \"The testbench simulated, but had errors. \"\n",
        "                \"Please fix the module. Output:\\n\" + sim_out\n",
        "            )\n",
        "            self.rank = 0.0\n",
        "            print(\"Simulation failed\")\n",
        "            return\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
        "################################################################################\n",
        "# Define the cost per million tokens\n",
        "COST_PER_MILLION_INPUT_TOKENS_GPT4 = 5.0\n",
        "COST_PER_MILLION_OUTPUT_TOKENS_GPT4 = 15.0\n",
        "\n",
        "COST_PER_MILLION_INPUT_TOKENS_GPT4M = 0.15\n",
        "COST_PER_MILLION_OUTPUT_TOKENS_GPT4M = 0.60\n",
        "\n",
        "COST_PER_MILLION_INPUT_TOKENS_GPT = 0.50\n",
        "COST_PER_MILLION_OUTPUT_TOKENS_GPT = 1.50\n",
        "\n",
        "COST_PER_MILLION_INPUT_TOKENS_CLAUDE = 0.25\n",
        "COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE = 1.25\n",
        "\n",
        "# Function to count tokens\n",
        "def count_tokens(model_family, text):\n",
        "    #print(f\"Counting tokens for string: {text}\")\n",
        "    if model_family == \"GPT\" or model_family == \"GPT4\" or model_family == \"GPT4M\":\n",
        "        return len(tiktoken.get_encoding(\"cl100k_base\").encode(text))\n",
        "    elif model_family == \"claude\":\n",
        "        return anthropic.Client().count_tokens(text)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
        "\n",
        "\n",
        "def calculate_cost(model_family,input_strings,output_strings):\n",
        "    input_tokens = sum(count_tokens(model_family, text) for text in input_strings)\n",
        "    output_tokens = sum(count_tokens(model_family, text) for text in output_strings)\n",
        "    if model_family == \"GPT\":\n",
        "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT\n",
        "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT\n",
        "    elif model_family == \"GPT4\":\n",
        "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4\n",
        "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4\n",
        "    elif model_family == \"GPT4M\":\n",
        "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_GPT4M\n",
        "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_GPT4M\n",
        "    elif model_family == \"claude\":\n",
        "        cost_input = (input_tokens / 1_000_000) * COST_PER_MILLION_INPUT_TOKENS_CLAUDE\n",
        "        cost_output = (output_tokens / 1_000_000) * COST_PER_MILLION_OUTPUT_TOKENS_CLAUDE\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model family: {model_family}\")\n",
        "    total_cost = cost_input + cost_output\n",
        "    return total_cost, input_tokens, output_tokens\n",
        "\n",
        "\n",
        "def format_message(role, content):\n",
        "    return f\"\\n{{role : '{role}', content : '{content}'}}\"\n",
        "\n",
        "def find_verilog_modules(markdown_string):\n",
        "    \"\"\"Find all Verilog modules in the markdown string\"\"\"\n",
        "    # Regular expression to match module definitions with or without parameters\n",
        "    module_pattern = r'\\bmodule\\b\\s+[\\w\\\\_]+\\s*(?:#\\s*\\([^)]*\\))?\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "    # Find all matches in the input string\n",
        "    matches = re.findall(module_pattern, markdown_string, re.DOTALL)\n",
        "    # Process matches to replace escaped characters\n",
        "    processed_matches = [match.replace('\\\\_', '_') for match in matches]\n",
        "    return processed_matches\n",
        "\n",
        "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
        "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
        "    code_match = find_verilog_modules(markdown_string)\n",
        "\n",
        "    if not code_match:\n",
        "        print(\"No code blocks found in response\")\n",
        "        exit(3)\n",
        "\n",
        "    # Open the specified file to write the code blocks\n",
        "    with open(filename, 'w') as file:\n",
        "        for code_block in code_match:\n",
        "            file.write(code_block)\n",
        "            file.write('\\n')\n",
        "\n",
        "\n",
        "def generate_verilog(conv, model_type, model_id=\"\"):\n",
        "    if model_type == \"ChatGPT\":\n",
        "        model = ChatGPT()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    return(model.generate(conv))\n",
        "\n",
        "def compile_iverilog(outdir,module,compiler_cmd,response:LLMResponse):\n",
        "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
        "\n",
        "    filename = os.path.join(outdir,module+\".sv\")\n",
        "    write_code_blocks_to_file(response.parsed_text, \"module\", filename)\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt < 3:\n",
        "        try:\n",
        "            proc = subprocess.run(compiler_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
        "            return proc.returncode, proc.stderr, proc.stdout\n",
        "        except subprocess.TimeoutExpired:\n",
        "            attempt += 1\n",
        "            if attempt >= 3:\n",
        "                raise ValueError(\"Compilation attempts timed out\")\n",
        "\n",
        "def simulate_iverilog(simulation_cmd):\n",
        "    \"\"\"Compile the Verilog module and return the output\"\"\"\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt < 3:\n",
        "        try:\n",
        "            proc = subprocess.run(simulation_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)\n",
        "            return proc.returncode, proc.stderr, proc.stdout\n",
        "        except subprocess.TimeoutExpired:\n",
        "            attempt += 1\n",
        "            if attempt >= 3:\n",
        "                raise ValueError(\"Simulation attempts timed out\")\n",
        "\n",
        "def generate_verilog_responses(conv, model_type, model_id=\"\", num_candidates=1):\n",
        "    match model_type:\n",
        "        case \"ChatGPT\":\n",
        "            model = ChatGPT(model_id)\n",
        "        case _:\n",
        "            raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    return(model.generate(conversation=conv, num_candidates=num_candidates))\n",
        "\n",
        "def get_iteration_ensemble(iteration, ensemble_config):\n",
        "\n",
        "    sorted_ensemble = sorted(ensemble_config.values(), key=lambda x: x['start_iteration'], reverse=True)\n",
        "\n",
        "    family = None\n",
        "    model_id = None\n",
        "    for ensemble_info in sorted_ensemble:\n",
        "        if iteration >= ensemble_info['start_iteration']:\n",
        "            family = ensemble_info['model_family']\n",
        "            model_id = ensemble_info['model_id']\n",
        "            break\n",
        "    return family, model_id\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, model_id=\"\", num_candidates=5, outdir=\"\", log=None, ensemble_config={}):\n",
        "\n",
        "    if outdir != \"\":\n",
        "        outdir = outdir + \"/\"\n",
        "\n",
        "    conv = Conversation(log_file=log)\n",
        "\n",
        "    #conv.add_message(\"system\", \"You are a Verilog engineering tool. Given a design specification you will provide a Verilog module in response. Given errors in that design you will provide a completed fixed module. Only complete functional models should be given. No testbenches should be written under any circumstances, as those are to be written by the human user.\")\n",
        "    conv.add_message(\"system\", \"\"\"\n",
        "    You are an expert Verilog RTL engineer.\n",
        "\n",
        "    Strict rules:\n",
        "    - Use plain Verilog-2001 only.\n",
        "    - Do NOT use typedef.\n",
        "    - Do NOT use enum.\n",
        "    - Do NOT use logic type.\n",
        "    - Use reg and wire only.\n",
        "    - Use parameter for state encoding.\n",
        "    - Use posedge clk.\n",
        "    - Active-low synchronous reset.\n",
        "    - Fully synthesizable.\n",
        "\n",
        "    You must implement an 8-state deterministic FSM.\n",
        "\n",
        "    Detect exactly this sequence:\n",
        "\n",
        "    001 → 101 → 110 → 000 → 110 → 110 → 011 → 101\n",
        "\n",
        "    CRITICAL:\n",
        "    The output sequence_found must be asserted\n",
        "    in the SAME clock cycle that the final 3'b101 is received.\n",
        "\n",
        "    It must NOT be delayed to the next clock cycle.\n",
        "\n",
        "    Return ONLY one complete Verilog module inside triple backticks.\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #with open(testbench, 'r') as file: testbench_text = file.read()\n",
        "    #full_prompt = design_prompt + \"\\n\\nThe module will be tested with the following testbench:\\n\\n\" + testbench_text + \"\\n\\n\"\n",
        "\n",
        "    with open(testbench, 'r') as file:\n",
        "      testbench_text = file.read()\n",
        "\n",
        "    full_prompt = design_prompt + \"\\n\\nThe module will be tested with the following testbench:\\n\\n\" + testbench_text\n",
        "\n",
        "    conv.add_message(\"user\", full_prompt)\n",
        "\n",
        "\n",
        "    success = False\n",
        "    timeout = False\n",
        "\n",
        "    iterations = 0\n",
        "\n",
        "    global_max_response = LLMResponse(-3,-3,\"\")\n",
        "\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    while not (success or timeout):\n",
        "\n",
        "\n",
        "        if ensemble_config:\n",
        "            print(f\"Getting model from ensemble\")\n",
        "            model_type, model_id = get_iteration_ensemble(iterations, ensemble_config)\n",
        "\n",
        "        print(f\"Iteration: {iterations}\")\n",
        "        print(f\"Model type: {model_type}\")\n",
        "        print(f\"Model ID: {model_id}\")\n",
        "        print(f\"Number of responses: {num_candidates}\")\n",
        "\n",
        "        response_texts=generate_verilog_responses(conv, model_type, model_id, num_candidates=num_candidates)\n",
        "\n",
        "        responses = [LLMResponse(iterations,response_num,response_text) for response_num,response_text in enumerate(response_texts)]\n",
        "        for index, response in enumerate(responses):\n",
        "\n",
        "            response_outdir = os.path.join(outdir, f\"iter{str(iterations)}/response{index}/\")\n",
        "            if not os.path.exists(response_outdir):\n",
        "                os.makedirs(response_outdir)\n",
        "\n",
        "\n",
        "            response_cost = 0\n",
        "            input_tokens = 0\n",
        "            output_tokens = 0\n",
        "\n",
        "            response.parse_verilog()\n",
        "            if response.parsed_text == \"\":\n",
        "                response.rank = -2\n",
        "                response.message = \"No modules found in response\"\n",
        "            else:\n",
        "                response.calculate_rank(response_outdir, module, testbench)\n",
        "\n",
        "            input_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'user' or msg['role'] == 'system']\n",
        "            output_messages = [msg['content'] for msg in conv.get_messages() if msg['role'] == 'assistant']\n",
        "            output_messages.append(response.parsed_text)\n",
        "            if model_type == \"ChatGPT\" and model_id == \"gpt-4o\":\n",
        "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4\",input_messages,output_messages)\n",
        "            elif model_type == \"ChatGPT\" and model_id == \"gpt-4o-mini\":\n",
        "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT4M\",input_messages,output_messages)\n",
        "            elif model_type == \"ChatGPT\" and model_id == \"gpt-3.5-turbo\":\n",
        "                response_cost, input_tokens, output_tokens = calculate_cost(\"GPT\",input_messages,output_messages)\n",
        "            elif model_type == \"Claude\":\n",
        "                response_cost, input_tokens, output_tokens = calculate_cost(\"claude\",input_messages,output_messages)\n",
        "\n",
        "\n",
        "            print(f\"Cost for response {index}: ${response_cost:.10f}\")\n",
        "\n",
        "            with open(os.path.join(response_outdir,f\"log.txt\"), 'w') as file:\n",
        "                file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
        "                file.write(format_message(\"assistant\", response.full_text))\n",
        "                file.write('\\n\\n Iteration rank: ' + str(response.rank) + '\\n') ## FIX\n",
        "\n",
        "                file.write(f\"\\n Model: {model_id}\")\n",
        "                file.write(f\"\\n Input tokens: {input_tokens}\")\n",
        "                file.write(f\"\\n Output tokens: {output_tokens}\")\n",
        "                file.write(f\"\\nTotal cost: ${response_cost:.10f}\\n\")\n",
        "\n",
        "        ## RANK RESPONSES\n",
        "        max_rank_response = max(responses, key=lambda resp: (resp.rank, -resp.parsed_length))\n",
        "        if max_rank_response.rank > global_max_response.rank:\n",
        "            global_max_response = max_rank_response\n",
        "        elif max_rank_response.rank == global_max_response.rank and max_rank_response.parsed_length > global_max_response.parsed_length:\n",
        "            global_max_response = max_rank_response\n",
        "\n",
        "        print(f\"Response ranks: {[resp.rank for resp in responses]}\")\n",
        "        print(f\"Response lengths: {[resp.parsed_length for resp in responses]}\")\n",
        "\n",
        "        conv.add_message(\"assistant\", max_rank_response.parsed_text)\n",
        "\n",
        "        if max_rank_response.rank == 1:\n",
        "            success = True\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "\n",
        "\n",
        "        if not success:\n",
        "            if iterations > 0:\n",
        "                conv.remove_message(2)\n",
        "                conv.remove_message(2)\n",
        "\n",
        "            #with open(testbench, 'r') as file: testbench_text = file.read()\n",
        "            #message = message + \"\\n\\nThe testbench used for these results is as follows:\\n\\n\" + testbench_text\n",
        "            #message = message + \"\\n\\nCommon sources of errors are as follows:\\n\\t- Use of SystemVerilog syntax which is not valid with iverilog\\n\\t- The reset must be made asynchronous active-low\\n\"\n",
        "            feedback_message = f\"\"\"\n",
        "            The previous FSM failed simulation.\n",
        "\n",
        "            Simulation error:\n",
        "            {max_rank_response.feedback}\n",
        "\n",
        "            The FSM reached the final state but did NOT assert\n",
        "            sequence_found in the same clock cycle as the final 3'b101 input.\n",
        "\n",
        "            Fix the output logic so that:\n",
        "            - sequence_found is asserted in the SAME cycle\n",
        "              that data == 3'b101 in state S7.\n",
        "            - It must NOT be delayed to the next cycle.\n",
        "            - It must be deasserted otherwise.\n",
        "\n",
        "            Return the full corrected module.\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            conv.add_message(\"user\", feedback_message)\n",
        "\n",
        "\n",
        "        if iterations >= max_iterations:\n",
        "            timeout = True\n",
        "\n",
        "        iterations += 1\n",
        "\n",
        "    return global_max_response\n",
        "\n"
      ],
      "metadata": {
        "id": "L5fN-IArOHQ8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  \"general\": {\n",
        "    \"name\": \"sequence_detector\",\n",
        "    \"prompt\": \"prompt.txt\",\n",
        "    \"testbench\": \"sequence_detector_tb.v\",\n",
        "    \"iterations\": 6,\n",
        "    \"num_candidates\": 3,\n",
        "    \"outdir\": \"out\",\n",
        "    \"log\": \"trajectory.log\",\n",
        "    \"model_family\": \"ChatGPT\",\n",
        "    \"model_id\": \"gpt-4o\"\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "VMcxHDTB5yGC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(json.dumps(config, indent=2))\n",
        "print(\"config.json written successfully.\")"
      ],
      "metadata": {
        "id": "FOcIqEEz51Es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12acf03e-3f6a-4541-b3a7-e5f6609b1499"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"general\": {\n",
            "    \"name\": \"sequence_detector\",\n",
            "    \"prompt\": \"prompt.txt\",\n",
            "    \"testbench\": \"sequence_detector_tb.v\",\n",
            "    \"iterations\": 6,\n",
            "    \"num_candidates\": 3,\n",
            "    \"outdir\": \"out\",\n",
            "    \"log\": \"trajectory.log\",\n",
            "    \"model_family\": \"ChatGPT\",\n",
            "    \"model_id\": \"gpt-4o\"\n",
            "  }\n",
            "}\n",
            "config.json written successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "design_prompt = \"\"\"\n",
        "Implement module sequence_detector.\n",
        "\n",
        "Interface:\n",
        "\n",
        "module sequence_detector(\n",
        "    input  wire clk,\n",
        "    input  wire reset_n,\n",
        "    input  wire [2:0] data,\n",
        "    output reg sequence_found\n",
        ");\n",
        "\n",
        "Design an 8-state FSM (S0–S7).\n",
        "\n",
        "Transitions:\n",
        "S0: expect 001\n",
        "S1: expect 101\n",
        "S2: expect 110\n",
        "S3: expect 000\n",
        "S4: expect 110\n",
        "S5: expect 110\n",
        "S6: expect 011\n",
        "S7: expect 101\n",
        "\n",
        "Only when the final 101 is received in S7,\n",
        "sequence_found must be 1 for one cycle.\n",
        "\n",
        "All other cycles must output 0.\n",
        "Return to S0 on mismatch.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Write prompt to file (required by config)\n",
        "with open(\"prompt.txt\", \"w\") as f:\n",
        "    f.write(design_prompt)\n",
        "print(\"prompt.txt written.\")"
      ],
      "metadata": {
        "id": "FaYUENLD53yU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df68af3a-d037-4e6e-e51b-49559d534778"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt.txt written.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9uyjOlwGS8"
      },
      "source": [
        "## Testbench — `sequence_detector_tb.v`\n",
        "\n",
        "The testbench `sequence_detector_tb.v` is the official course-supplied benchmark\n",
        "from `LLM4ChipDesign/VerilogGenBenchmark/TestBench/`. It drives the module through\n",
        "the full 8-step sequence across 8 clock cycles:\n",
        "\n",
        "| Cycle | Input `data` | Expected `sequence_found` |\n",
        "|-------|-------------|--------------------------|\n",
        "| 1 | `001` | `0` |\n",
        "| 2 | `101` | `0` |\n",
        "| 3 | `110` | `0` |\n",
        "| 4 | `000` | `0` |\n",
        "| 5 | `110` | `0` |\n",
        "| 6 | `110` | `0` |\n",
        "| 7 | `011` | `0` |\n",
        "| 8 | `101` | `1` ← must assert same cycle |\n",
        "\n",
        "It also checks **mismatch recovery** — a wrong input mid-sequence must return the FSM to S0.\n",
        "A pass prints `\"All test cases passed.\"` The top-level module is `tb_sequence_detector`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o sequence_detector_tb.v \\\n",
        "https://raw.githubusercontent.com/FCHXWH823/LLM4ChipDesign/main/VerilogGenBenchmark/TestBench/sequence_detector_tb.v"
      ],
      "metadata": {
        "id": "QbNjDfcf6tBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f9cb94-498d-48ce-d597-a03af4ad945b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2114  100  2114    0     0  14877      0 --:--:-- --:--:-- --:--:-- 14992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sequence_detector_tb.v\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "Lhwf5IHLcgrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1154127-15ab-4793-f2ee-6177410e5523"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`timescale 1ns/1ps\n",
            "\n",
            "module tb_sequence_detector();\n",
            "    reg clk;\n",
            "    reg reset_n;\n",
            "    reg [2:0] data;\n",
            "    wire sequence_found;\n",
            "\n",
            "    // Instantiate the sequence_detector module\n",
            "    sequence_detector dut (\n",
            "        .clk(clk),\n",
            "        .reset_n(reset_n),\n",
            "        .data(data),\n",
            "        .sequence_found(sequence_found)\n",
            "    );\n",
            "\n",
            "    // Clock generation\n",
            "    always begin\n",
            "        #5 clk = ~clk;\n",
            "    end\n",
            "\n",
            "    // Test stimulus task\n",
            "    task apply_stimulus;\n",
            "        input [2:0] data_value;\n",
            "        input integer delay_cycles;\n",
            "        begin\n",
            "            data <= data_value;\n",
            "            repeat (delay_cycles) @(posedge clk);\n",
            "        end\n",
            "    endtask\n",
            "\n",
            "    // Check output task\n",
            "    task check_output;\n",
            "        input integer cycle;\n",
            "        input expected_value;\n",
            "        begin\n",
            "            if (sequence_found !== expected_value) begin\n",
            "                $display(\"Error: Cycle %0d, Expected: %b, Got: %b\", cycle, expected_value, sequence_found);\n",
            "                $finish;\n",
            "            end\n",
            "        end\n",
            "    endtask\n",
            "\n",
            "    // Testbench stimulus and checking\n",
            "    initial begin\n",
            "        // Initialize signals\n",
            "        clk <= 0;\n",
            "        reset_n <= 0;\n",
            "        data <= 3'b000;\n",
            "\n",
            "        // Apply reset\n",
            "        @(posedge clk);\n",
            "        reset_n <= 1;\n",
            "\n",
            "        // Test case: Correct sequence\n",
            "        apply_stimulus(3'b001, 1); check_output(1, 1'b0);\n",
            "        apply_stimulus(3'b101, 1); check_output(2, 1'b0);\n",
            "        apply_stimulus(3'b110, 1); check_output(3, 1'b0);\n",
            "        apply_stimulus(3'b000, 1); check_output(4, 1'b0);\n",
            "        apply_stimulus(3'b110, 1); check_output(5, 1'b0);\n",
            "        apply_stimulus(3'b110, 1); check_output(6, 1'b0);\n",
            "        apply_stimulus(3'b011, 1); check_output(7, 1'b0);\n",
            "        apply_stimulus(3'b101, 1); check_output(8, 1'b1);\n",
            "\n",
            "        // Test case: Incorrect sequence\n",
            "        apply_stimulus(3'b001, 1); check_output(9, 1'b0);\n",
            "        apply_stimulus(3'b101, 1); check_output(10, 1'b0);\n",
            "        apply_stimulus(3'b010, 1); check_output(11, 1'b0);\n",
            "        apply_stimulus(3'b000, 1); check_output(12, 1'b0);\n",
            "\n",
            "        // Indicate successful test completion\n",
            "        $display(\"All test cases passed.\");\n",
            "        $finish;\n",
            "    end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKtdnNybwGS8"
      },
      "source": [
        "## AutoChip Trajectory\n",
        "\n",
        "**Model:** `gpt-4o` | **Max iterations:** 6 | **Candidates per iteration:** 3 | **Temperature:** default\n",
        "\n",
        "**Key design challenge:** The repeated `110` inputs at positions 5 and 6 require **two distinct\n",
        "states** (S4 and S5). A single state with a self-loop on `110` loses count of how many\n",
        "consecutive `110`s have been seen, causing `sequence_found` to never assert on the final `101`.\n",
        "\n",
        "**Iteration 0:** All 3 candidates failed simulation.\n",
        "Output: `Error: Cycle 8, Expected: 1, Got: 0` — `sequence_found` was never asserted.\n",
        "Likely cause: LLM merged S4/S5 into a single state or used delayed output logic.\n",
        "Feedback sent: *\"The testbench simulated, but had errors.\"*\n",
        "\n",
        "**Iteration 1:** All 3 candidates passed immediately.\n",
        "`Response ranks: [1.0, 1.0, 1.0]` — `All test cases passed.`\n",
        "LLM correctly implemented separate S4 and S5 states with combinational\n",
        "output asserting `sequence_found` in the same cycle as the final `101`.\n",
        "\n",
        "AutoChip terminated after **iteration 1** with `Final Best Rank: 1.0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_values, ensemble_config, logfile = parse_args_and_config()\n",
        "\n",
        "result = verilog_loop(\n",
        "    design_prompt=design_prompt,\n",
        "    module=config_values[\"name\"],\n",
        "    testbench=config_values[\"testbench\"],\n",
        "    max_iterations=config_values[\"iterations\"],\n",
        "    model_type=config_values[\"model_family\"],\n",
        "    model_id=config_values[\"model_id\"],\n",
        "    num_candidates=config_values[\"num_candidates\"],\n",
        "    outdir=config_values[\"outdir\"],\n",
        "    log=logfile,\n",
        "    ensemble_config=ensemble_config\n",
        ")\n",
        "\n",
        "print(\"Final Best Rank:\", result.rank)"
      ],
      "metadata": {
        "id": "bSpBKaBhE3R1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad055495-a0a6-43b7-cdd1-a96cdedb257c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "Model type: ChatGPT\n",
            "Model ID: gpt-4o\n",
            "Number of responses: 3\n",
            "Simulation output:\n",
            "Error: Cycle 8, Expected: 1, Got: 0\n",
            "\n",
            "Simulation failed\n",
            "Cost for response 0: $0.0114500000\n",
            "Simulation output:\n",
            "Error: Cycle 8, Expected: 1, Got: 0\n",
            "\n",
            "Simulation failed\n",
            "Cost for response 1: $0.0116600000\n",
            "Simulation output:\n",
            "Error: Cycle 8, Expected: 1, Got: 0\n",
            "\n",
            "Simulation failed\n",
            "Cost for response 2: $0.0114500000\n",
            "Response ranks: [0.0, 0.0, 0.0]\n",
            "Response lengths: [1436, 1453, 1436]\n",
            "Iteration: 1\n",
            "Model type: ChatGPT\n",
            "Model ID: gpt-4o\n",
            "Number of responses: 3\n",
            "Simulation output:\n",
            "All test cases passed.\n",
            "\n",
            "Testbench ran successfully\n",
            "Cost for response 0: $0.0183500000\n",
            "Simulation output:\n",
            "All test cases passed.\n",
            "\n",
            "Testbench ran successfully\n",
            "Cost for response 1: $0.0183500000\n",
            "Simulation output:\n",
            "All test cases passed.\n",
            "\n",
            "Testbench ran successfully\n",
            "Cost for response 2: $0.0183500000\n",
            "Response ranks: [1.0, 1.0, 1.0]\n",
            "Response lengths: [1331, 1331, 1331]\n",
            "Final Best Rank: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xERfeI3wGS9",
        "outputId": "ce28d5d2-d235-40aa-c7c2-976cb1d86ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed.\n"
          ]
        }
      ],
      "source": [
        "# Explicit compile and simulate commands (mirrors AutoChip internal calls)\n",
        "!iverilog -Wall -Winfloop -Wno-timescale -g2012 \\\n",
        "    -s tb_sequence_detector \\\n",
        "    -o out/sequence_detector_final.vvp \\\n",
        "    out/iter1/response0/sequence_detector.sv \\\n",
        "    sequence_detector_tb.v\n",
        "\n",
        "!vvp -n out/sequence_detector_final.vvp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmNhTu4wGS9"
      },
      "source": [
        "## Part I(b) — Manual RTL Design\n",
        "\n",
        "**Design choices:**\n",
        "- **`parameter` state encoding** — 8 states S0–S7 as plain integers. No `typedef`/`enum`\n",
        "  to stay strict Verilog-2001 compliant with iverilog.\n",
        "- **S4 and S5 are separate states** — both expect `110` but must be counted independently.\n",
        "  A single state with a self-loop cannot distinguish \"first `110`\" from \"second `110`\",\n",
        "  which breaks the sequence count at positions 5 and 6.\n",
        "- **Synchronous active-low reset** — `if (!reset_n)` inside `always @(posedge clk)`\n",
        "  resets state to S0 on the clock edge, matching the testbench expectation.\n",
        "- **Combinational output** — `sequence_found` is driven by an `always @(*)` block that\n",
        "  checks `state == S7 && data == 3'b101`, satisfying the same-cycle assertion requirement.\n",
        "- **Mismatch → S0** — any unexpected input in any state returns the FSM to S0,\n",
        "  matching the testbench recovery checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eatox8E2wGS9",
        "outputId": "5ed549c4-a7ac-411f-f7ea-fe5f42345c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// Manual RTL — sequence_detector_manual.v\n",
            "// Part I(b): Hand-written Verilog-2001, 8-state sequence detector FSM\n",
            "\n",
            "module sequence_detector (\n",
            "    input  wire       clk,\n",
            "    input  wire       reset_n,\n",
            "    input  wire [2:0] data,\n",
            "    output reg        sequence_found\n",
            ");\n",
            "\n",
            "    // State encoding — plain parameters, Verilog-2001 compliant\n",
            "    parameter S0 = 3'd0, S1 = 3'd1, S2 = 3'd2, S3 = 3'd3;\n",
            "    parameter S4 = 3'd4, S5 = 3'd5, S6 = 3'd6, S7 = 3'd7;\n",
            "\n",
            "    reg [2:0] state, next_state;\n",
            "\n",
            "    // State register — synchronous active-low reset\n",
            "    always @(posedge clk) begin\n",
            "        if (!reset_n)\n",
            "            state <= S0;\n",
            "        else\n",
            "            state <= next_state;\n",
            "    end\n",
            "\n",
            "    // Next-state logic\n",
            "    always @(*) begin\n",
            "        case (state)\n",
            "            S0: next_state = (data == 3'b001) ? S1 : S0;\n",
            "            S1: next_state = (data == 3'b101) ? S2 : S0;\n",
            "            S2: next_state = (data == 3'b110) ? S3 : S0;\n",
            "            S3: next_state = (data == 3'b000) ? S4 : S0;\n",
            "            S4: next_state = (data == 3'b110) ? S5 : S0;\n",
            "            S5: next_state = (data == 3'b110) ? S6 : S0;\n",
            "            S6: next_state = (data == 3'b011) ? S7 : S0;\n",
            "            S7: next_state = S0;\n",
            "            default: next_state = S0;\n",
            "        endcase\n",
            "    end\n",
            "\n",
            "    // Output logic — combinational, same-cycle assertion\n",
            "    always @(*) begin\n",
            "        sequence_found = (state == S7 && data == 3'b101) ? 1'b1 : 1'b0;\n",
            "    end\n",
            "\n",
            "endmodule\n",
            "\n"
          ]
        }
      ],
      "source": [
        "manual_rtl = \"\"\"\n",
        "// Manual RTL — sequence_detector_manual.v\n",
        "// Part I(b): Hand-written Verilog-2001, 8-state sequence detector FSM\n",
        "\n",
        "module sequence_detector (\n",
        "    input  wire       clk,\n",
        "    input  wire       reset_n,\n",
        "    input  wire [2:0] data,\n",
        "    output reg        sequence_found\n",
        ");\n",
        "\n",
        "    // State encoding — plain parameters, Verilog-2001 compliant\n",
        "    parameter S0 = 3'd0, S1 = 3'd1, S2 = 3'd2, S3 = 3'd3;\n",
        "    parameter S4 = 3'd4, S5 = 3'd5, S6 = 3'd6, S7 = 3'd7;\n",
        "\n",
        "    reg [2:0] state, next_state;\n",
        "\n",
        "    // State register — synchronous active-low reset\n",
        "    always @(posedge clk) begin\n",
        "        if (!reset_n)\n",
        "            state <= S0;\n",
        "        else\n",
        "            state <= next_state;\n",
        "    end\n",
        "\n",
        "    // Next-state logic\n",
        "    always @(*) begin\n",
        "        case (state)\n",
        "            S0: next_state = (data == 3'b001) ? S1 : S0;\n",
        "            S1: next_state = (data == 3'b101) ? S2 : S0;\n",
        "            S2: next_state = (data == 3'b110) ? S3 : S0;\n",
        "            S3: next_state = (data == 3'b000) ? S4 : S0;\n",
        "            S4: next_state = (data == 3'b110) ? S5 : S0;\n",
        "            S5: next_state = (data == 3'b110) ? S6 : S0;\n",
        "            S6: next_state = (data == 3'b011) ? S7 : S0;\n",
        "            S7: next_state = S0;\n",
        "            default: next_state = S0;\n",
        "        endcase\n",
        "    end\n",
        "\n",
        "    // Output logic — combinational, same-cycle assertion\n",
        "    always @(*) begin\n",
        "        sequence_found = (state == S7 && data == 3'b101) ? 1'b1 : 1'b0;\n",
        "    end\n",
        "\n",
        "endmodule\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sequence_detector_manual.v\", \"w\") as f:\n",
        "    f.write(manual_rtl)\n",
        "print(manual_rtl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTuMBeXSwGS-",
        "outputId": "8def5181-a8c6-450a-c8ca-af650d1da1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed.\n"
          ]
        }
      ],
      "source": [
        "# Verify manual RTL with the same course-supplied testbench\n",
        "!iverilog -Wall -Winfloop -Wno-timescale -g2012 \\\n",
        "    -s tb_sequence_detector \\\n",
        "    -o out/manual.vvp \\\n",
        "    sequence_detector_manual.v \\\n",
        "    sequence_detector_tb.v\n",
        "\n",
        "!vvp -n out/manual.vvp"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}